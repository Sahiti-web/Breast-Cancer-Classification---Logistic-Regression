import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (confusion_matrix, classification_report, 
                             accuracy_score, precision_score, recall_score, 
                             f1_score, roc_auc_score, roc_curve)
import warnings
warnings.filterwarnings('ignore')

sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)

print("BREAST CANCER CLASSIFICATION - LOGISTIC REGRESSION")
print("-" * 60)

# Load dataset
# Using sklearn's built-in breast cancer dataset
from sklearn.datasets import load_breast_cancer

data = load_breast_cancer()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target

print("\n1. DATASET OVERVIEW")
print("-" * 60)
print(f"Dataset Shape: {df.shape[0]} rows × {df.shape[1]} columns")
print(f"\nTarget Variable: {data.target_names}")
print("0 = Malignant (Cancer)")
print("1 = Benign (No Cancer)")

print("\nFirst 5 rows:")
print(df.head())

print("\nDataset Info:")
print(df.info())

print("\nBasic Statistics:")
print(df.describe())

# Check class distribution
print("\n2. CLASS DISTRIBUTION")
print("-" * 60)
class_counts = df['target'].value_counts()
print("\nClass counts:")
print(f"Benign (1): {class_counts[1]} ({class_counts[1]/len(df)*100:.1f}%)")
print(f"Malignant (0): {class_counts[0]} ({class_counts[0]/len(df)*100:.1f}%)")

# Visualize class distribution
plt.figure(figsize=(8, 6))
class_counts.plot(kind='bar', color=['#ff6b6b', '#51cf66'])
plt.title('Class Distribution', fontsize=14, fontweight='bold')
plt.xlabel('Class (0=Malignant, 1=Benign)')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('class_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

# Check for missing values
print("\n3. MISSING VALUES CHECK")
print("-" * 60)
missing = df.isnull().sum().sum()
if missing > 0:
    print(f"Missing values found: {missing}")
else:
    print("No missing values found!")

# Feature selection (using top features for simplicity)
print("\n4. FEATURE SELECTION")
print("-" * 60)

# For demonstration, let's use a few important features
# In practice, you might use all features or do feature selection
selected_features = ['mean radius', 'mean texture', 'mean perimeter', 
                     'mean area', 'mean smoothness']

print(f"Using {len(selected_features)} features:")
for i, feature in enumerate(selected_features, 1):
    print(f"  {i}. {feature}")

X = df[selected_features]
y = df['target']

print(f"\nX shape: {X.shape}")
print(f"y shape: {y.shape}")

# Split data into training and testing sets
print("\n5. TRAIN-TEST SPLIT")
print("-" * 60)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Testing set: {X_test.shape[0]} samples")
print(f"Split ratio: 80-20 (stratified)")

print("\nClass distribution in training set:")
train_dist = pd.Series(y_train).value_counts()
print(f"Benign (1): {train_dist[1]} ({train_dist[1]/len(y_train)*100:.1f}%)")
print(f"Malignant (0): {train_dist[0]} ({train_dist[0]/len(y_train)*100:.1f}%)")

# Feature Scaling
print("\n6. FEATURE SCALING")
print("-" * 60)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Features standardized using StandardScaler")
print("Mean = 0, Standard Deviation = 1")

print("\nBefore scaling (first feature):")
print(f"  Mean: {X_train.iloc[:, 0].mean():.2f}")
print(f"  Std: {X_train.iloc[:, 0].std():.2f}")

print("\nAfter scaling (first feature):")
print(f"  Mean: {X_train_scaled[:, 0].mean():.2f}")
print(f"  Std: {X_train_scaled[:, 0].std():.2f}")

# Train Logistic Regression Model
print("\n7. MODEL TRAINING")
print("-" * 60)
model = LogisticRegression(random_state=42, max_iter=1000)
model.fit(X_train_scaled, y_train)
print("Logistic Regression model trained successfully!")

# Model coefficients
print("\n8. MODEL COEFFICIENTS")
print("-" * 60)
print(f"Intercept: {model.intercept_[0]:.4f}")
print("\nFeature Coefficients:")
for feature, coef in zip(selected_features, model.coef_[0]):
    print(f"  {feature}: {coef:.4f}")

# Make predictions
print("\n9. MAKING PREDICTIONS")
print("-" * 60)
y_pred_train = model.predict(X_train_scaled)
y_pred_test = model.predict(X_test_scaled)

# Get probability predictions
y_pred_proba_train = model.predict_proba(X_train_scaled)[:, 1]
y_pred_proba_test = model.predict_proba(X_test_scaled)[:, 1]

print("Predictions completed!")
print(f"\nSample probabilities (first 5 test samples):")
for i in range(min(5, len(y_pred_proba_test))):
    print(f"  Sample {i+1}: {y_pred_proba_test[i]:.4f} → Predicted: {y_pred_test[i]} (Actual: {y_test.iloc[i]})")

# Evaluate Model
print("\n10. MODEL EVALUATION")
print("-" * 60)

# Training set metrics
train_accuracy = accuracy_score(y_train, y_pred_train)
train_precision = precision_score(y_train, y_pred_train)
train_recall = recall_score(y_train, y_pred_train)
train_f1 = f1_score(y_train, y_pred_train)
train_roc_auc = roc_auc_score(y_train, y_pred_proba_train)

print("\nTraining Set Performance:")
print(f"  Accuracy:  {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
print(f"  Precision: {train_precision:.4f}")
print(f"  Recall:    {train_recall:.4f}")
print(f"  F1-Score:  {train_f1:.4f}")
print(f"  ROC-AUC:   {train_roc_auc:.4f}")

# Testing set metrics
test_accuracy = accuracy_score(y_test, y_pred_test)
test_precision = precision_score(y_test, y_pred_test)
test_recall = recall_score(y_test, y_pred_test)
test_f1 = f1_score(y_test, y_pred_test)
test_roc_auc = roc_auc_score(y_test, y_pred_proba_test)

print("\nTesting Set Performance:")
print(f"  Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
print(f"  Precision: {test_precision:.4f}")
print(f"  Recall:    {test_recall:.4f}")
print(f"  F1-Score:  {test_f1:.4f}")
print(f"  ROC-AUC:   {test_roc_auc:.4f}")

# Confusion Matrix
print("\n11. CONFUSION MATRIX")
print("-" * 60)
cm = confusion_matrix(y_test, y_pred_test)
print("\nConfusion Matrix:")
print(cm)

tn, fp, fn, tp = cm.ravel()
print(f"\nBreakdown:")
print(f"  True Negatives (TN):  {tn} - Correctly predicted Malignant")
print(f"  False Positives (FP): {fp} - Incorrectly predicted Benign")
print(f"  False Negatives (FN): {fn} - Incorrectly predicted Malignant")
print(f"  True Positives (TP):  {tp} - Correctly predicted Benign")

# Visualize Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Malignant', 'Benign'],
            yticklabels=['Malignant', 'Benign'])
plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.tight_layout()
plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

# Classification Report
print("\n12. DETAILED CLASSIFICATION REPORT")
print("-" * 60)
print("\n", classification_report(y_test, y_pred_test, 
                                  target_names=['Malignant', 'Benign']))

# ROC Curve
print("\n13. ROC CURVE")
print("-" * 60)
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_test)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {test_roc_auc:.3f})')
plt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Random Classifier')
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"ROC-AUC Score: {test_roc_auc:.4f}")
print("Interpretation: Model can distinguish between classes very well!")

# Sigmoid Function Visualization
print("\n14. SIGMOID FUNCTION")
print("-" * 60)

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

z = np.linspace(-10, 10, 200)
prob = sigmoid(z)

plt.figure(figsize=(10, 6))
plt.plot(z, prob, linewidth=2, color='blue')
plt.axhline(y=0.5, color='r', linestyle='--', label='Threshold = 0.5')
plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)
plt.xlabel('z (Linear Combination)', fontsize=12)
plt.ylabel('Probability', fontsize=12)
plt.title('Sigmoid Function', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.legend()
plt.tight_layout()
plt.savefig('sigmoid_function.png', dpi=300, bbox_inches='tight')
plt.show()

print("Sigmoid function maps any value to a probability between 0 and 1")

# Precision-Recall vs Threshold
print("\n15. THRESHOLD ANALYSIS")
print("-" * 60)

from sklearn.metrics import precision_recall_curve

precision_vals, recall_vals, threshold_vals = precision_recall_curve(y_test, y_pred_proba_test)

plt.figure(figsize=(10, 6))
plt.plot(threshold_vals, precision_vals[:-1], label='Precision', linewidth=2)
plt.plot(threshold_vals, recall_vals[:-1], label='Recall', linewidth=2)
plt.axvline(x=0.5, color='r', linestyle='--', label='Default Threshold (0.5)')
plt.xlabel('Threshold', fontsize=12)
plt.ylabel('Score', fontsize=12)
plt.title('Precision and Recall vs Threshold', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('precision_recall_threshold.png', dpi=300, bbox_inches='tight')
plt.show()

print("Default threshold: 0.5")
print("You can adjust this based on whether you want higher precision or recall")

# Feature Importance Visualization
print("\n16. FEATURE IMPORTANCE")
print("-" * 60)

coef_df = pd.DataFrame({
    'Feature': selected_features,
    'Coefficient': model.coef_[0]
}).sort_values('Coefficient', key=abs, ascending=False)

plt.figure(figsize=(10, 6))
colors = ['green' if x > 0 else 'red' for x in coef_df['Coefficient']]
plt.barh(coef_df['Feature'], coef_df['Coefficient'], color=colors)
plt.xlabel('Coefficient Value', fontsize=12)
plt.ylabel('Features', fontsize=12)
plt.title('Feature Coefficients (Importance)', fontsize=14, fontweight='bold')
plt.axvline(x=0, color='black', linestyle='--', linewidth=1)
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nMost important features:")
print(coef_df)

# Prediction Distribution
print("\n17. PREDICTION PROBABILITY DISTRIBUTION")
print("-" * 60)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Separate by actual class
benign_probs = y_pred_proba_test[y_test == 1]
malignant_probs = y_pred_proba_test[y_test == 0]

axes[0].hist(benign_probs, bins=30, alpha=0.7, color='green', edgecolor='black')
axes[0].axvline(x=0.5, color='r', linestyle='--', linewidth=2, label='Threshold')
axes[0].set_xlabel('Predicted Probability', fontsize=12)
axes[0].set_ylabel('Frequency', fontsize=12)
axes[0].set_title('Benign Cases (Actual = 1)', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

axes[1].hist(malignant_probs, bins=30, alpha=0.7, color='red', edgecolor='black')
axes[1].axvline(x=0.5, color='r', linestyle='--', linewidth=2, label='Threshold')
axes[1].set_xlabel('Predicted Probability', fontsize=12)
axes[1].set_ylabel('Frequency', fontsize=12)
axes[1].set_title('Malignant Cases (Actual = 0)', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('probability_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

# Testing different thresholds
print("\n18. TESTING DIFFERENT THRESHOLDS")
print("-" * 60)

thresholds_to_test = [0.3, 0.4, 0.5, 0.6, 0.7]
print("\nPerformance at different thresholds:\n")

for threshold in thresholds_to_test:
    y_pred_threshold = (y_pred_proba_test >= threshold).astype(int)
    acc = accuracy_score(y_test, y_pred_threshold)
    prec = precision_score(y_test, y_pred_threshold)
    rec = recall_score(y_test, y_pred_threshold)
    
    print(f"Threshold = {threshold}:")
    print(f"  Accuracy:  {acc:.4f}")
    print(f"  Precision: {prec:.4f}")
    print(f"  Recall:    {rec:.4f}")
    print()

# Summary
print("\n19. SUMMARY")
print("-" * 60)
print(f"Model: Logistic Regression")
print(f"Dataset: Breast Cancer Wisconsin")
print(f"Classes: Binary (Malignant vs Benign)")
print(f"Features: {len(selected_features)} selected features")
print(f"Training Samples: {len(X_train)}")
print(f"Testing Samples: {len(X_test)}")
print(f"\nBest Metrics (Test Set):")
print(f"  Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.1f}%)")
print(f"  Precision: {test_precision:.4f}")
print(f"  Recall:    {test_recall:.4f}")
print(f"  ROC-AUC:   {test_roc_auc:.4f}")
print(f"\nFalse Negatives: {fn} (Most critical - missed cancer cases)")
print(f"False Positives: {fp} (Less critical - false alarms)")

print("\nAll visualizations saved successfully!")
print("-" * 60)
